{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T05:46:35.798745Z",
     "start_time": "2025-05-09T05:46:35.189392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 1.1\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "# 设置原始数据路径\n",
    "data_dir = \"data/raw/CICDDoS2019\"\n",
    "csv_files = glob(os.path.join(data_dir, \"**/*.csv\"), recursive=True)\n",
    "\n",
    "column_counter = Counter()\n",
    "label_summary = {}\n",
    "\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(path, nrows=5000)  # 读取前5k行防止内存爆炸\n",
    "        filename = os.path.basename(path)\n",
    "        df.columns = [col.strip() for col in df.columns]  # 去除列名空格\n",
    "        print(f\"✅ 文件: {filename}\")\n",
    "        print(f\"   ▶️ 行数: {df.shape[0]}, 列数: {df.shape[1]}\")\n",
    "        print(f\"   🧱 缺失值总数: {df.isnull().sum().sum()}\")\n",
    "\n",
    "        # 累加完整列名频次\n",
    "        column_counter.update(df.columns)\n",
    "\n",
    "        # 标签列统计\n",
    "        label_col = [col for col in df.columns if 'label' in col.lower() or 'attack' in col.lower()]\n",
    "        if label_col:\n",
    "            label_counts = df[label_col[0]].value_counts()\n",
    "            label_summary[filename] = label_counts.to_dict()\n",
    "            print(f\"   🏷️ 标签列: {label_col[0]}, 分布: {label_counts.to_dict()}\")\n",
    "        else:\n",
    "            print(\"   ⚠️ 未找到标签列\")\n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取失败: {path}, 原因: {e}\")\n",
    "\n",
    "# 输出所有列名及频次\n",
    "print(\"\\n📊 所有标准化列名及出现频次（共 {} 个）：\".format(len(column_counter)))\n",
    "for col, count in column_counter.most_common():\n",
    "    print(f\"{col}: {count}\")\n"
   ],
   "id": "ed48ea67c8630855",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 文件: DrDoS_DNS.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 1\n",
      "   🏷️ 标签列: Label, 分布: {'DrDoS_DNS': 4413, 'BENIGN': 587}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: DrDoS_LDAP.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'DrDoS_LDAP': 4999, 'BENIGN': 1}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: DrDoS_MSSQL.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'DrDoS_MSSQL': 5000}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: DrDoS_NetBIOS.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'DrDoS_NetBIOS': 4991, 'BENIGN': 9}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: DrDoS_NTP.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 3\n",
      "   🏷️ 标签列: Label, 分布: {'BENIGN': 4222, 'DrDoS_NTP': 778}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: DrDoS_SNMP.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'DrDoS_SNMP': 4997, 'BENIGN': 3}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: DrDoS_SSDP.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'DrDoS_SSDP': 4973, 'BENIGN': 27}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: DrDoS_UDP.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'DrDoS_UDP': 5000}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: Syn.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 423\n",
      "   🏷️ 标签列: Label, 分布: {'Syn': 4998, 'BENIGN': 2}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: TFTP.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 618\n",
      "   🏷️ 标签列: Label, 分布: {'TFTP': 5000}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: UDPLag.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'UDP-lag': 5000}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: LDAP.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'NetBIOS': 4992, 'BENIGN': 8}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: MSSQL.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'LDAP': 4994, 'BENIGN': 6}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: NetBIOS.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'NetBIOS': 5000}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: Portmap.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'BENIGN': 3707, 'Portmap': 1293}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: Syn.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'Syn': 5000}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: UDP.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'MSSQL': 5000}\n",
      "------------------------------------------------------------\n",
      "✅ 文件: UDPLag.csv\n",
      "   ▶️ 行数: 5000, 列数: 88\n",
      "   🧱 缺失值总数: 0\n",
      "   🏷️ 标签列: Label, 分布: {'UDP': 5000}\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 所有标准化列名及出现频次（共 88 个）：\n",
      "Unnamed: 0: 18\n",
      "Flow ID: 18\n",
      "Source IP: 18\n",
      "Source Port: 18\n",
      "Destination IP: 18\n",
      "Destination Port: 18\n",
      "Protocol: 18\n",
      "Timestamp: 18\n",
      "Flow Duration: 18\n",
      "Total Fwd Packets: 18\n",
      "Total Backward Packets: 18\n",
      "Total Length of Fwd Packets: 18\n",
      "Total Length of Bwd Packets: 18\n",
      "Fwd Packet Length Max: 18\n",
      "Fwd Packet Length Min: 18\n",
      "Fwd Packet Length Mean: 18\n",
      "Fwd Packet Length Std: 18\n",
      "Bwd Packet Length Max: 18\n",
      "Bwd Packet Length Min: 18\n",
      "Bwd Packet Length Mean: 18\n",
      "Bwd Packet Length Std: 18\n",
      "Flow Bytes/s: 18\n",
      "Flow Packets/s: 18\n",
      "Flow IAT Mean: 18\n",
      "Flow IAT Std: 18\n",
      "Flow IAT Max: 18\n",
      "Flow IAT Min: 18\n",
      "Fwd IAT Total: 18\n",
      "Fwd IAT Mean: 18\n",
      "Fwd IAT Std: 18\n",
      "Fwd IAT Max: 18\n",
      "Fwd IAT Min: 18\n",
      "Bwd IAT Total: 18\n",
      "Bwd IAT Mean: 18\n",
      "Bwd IAT Std: 18\n",
      "Bwd IAT Max: 18\n",
      "Bwd IAT Min: 18\n",
      "Fwd PSH Flags: 18\n",
      "Bwd PSH Flags: 18\n",
      "Fwd URG Flags: 18\n",
      "Bwd URG Flags: 18\n",
      "Fwd Header Length: 18\n",
      "Bwd Header Length: 18\n",
      "Fwd Packets/s: 18\n",
      "Bwd Packets/s: 18\n",
      "Min Packet Length: 18\n",
      "Max Packet Length: 18\n",
      "Packet Length Mean: 18\n",
      "Packet Length Std: 18\n",
      "Packet Length Variance: 18\n",
      "FIN Flag Count: 18\n",
      "SYN Flag Count: 18\n",
      "RST Flag Count: 18\n",
      "PSH Flag Count: 18\n",
      "ACK Flag Count: 18\n",
      "URG Flag Count: 18\n",
      "CWE Flag Count: 18\n",
      "ECE Flag Count: 18\n",
      "Down/Up Ratio: 18\n",
      "Average Packet Size: 18\n",
      "Avg Fwd Segment Size: 18\n",
      "Avg Bwd Segment Size: 18\n",
      "Fwd Header Length.1: 18\n",
      "Fwd Avg Bytes/Bulk: 18\n",
      "Fwd Avg Packets/Bulk: 18\n",
      "Fwd Avg Bulk Rate: 18\n",
      "Bwd Avg Bytes/Bulk: 18\n",
      "Bwd Avg Packets/Bulk: 18\n",
      "Bwd Avg Bulk Rate: 18\n",
      "Subflow Fwd Packets: 18\n",
      "Subflow Fwd Bytes: 18\n",
      "Subflow Bwd Packets: 18\n",
      "Subflow Bwd Bytes: 18\n",
      "Init_Win_bytes_forward: 18\n",
      "Init_Win_bytes_backward: 18\n",
      "act_data_pkt_fwd: 18\n",
      "min_seg_size_forward: 18\n",
      "Active Mean: 18\n",
      "Active Std: 18\n",
      "Active Max: 18\n",
      "Active Min: 18\n",
      "Idle Mean: 18\n",
      "Idle Std: 18\n",
      "Idle Max: 18\n",
      "Idle Min: 18\n",
      "SimillarHTTP: 18\n",
      "Inbound: 18\n",
      "Label: 18\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T05:47:21.980563Z",
     "start_time": "2025-05-09T05:47:21.965238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 1.2\n",
    "\n",
    "import json\n",
    "\n",
    "# 排除掉标签列\n",
    "standard_columns = [col for col in column_counter if col != \"Label\"]\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open(\"feature_columns.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(standard_columns, f, indent=2)\n",
    "\n",
    "print(f\"✅ 已保存标准字段列 {len(standard_columns)} 项 到 feature_columns.json\")\n"
   ],
   "id": "84557bc4063d4936",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已保存标准字段列 87 项 到 feature_columns.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T06:29:09.418491Z",
     "start_time": "2025-05-09T06:17:40.936540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 1.3（最终增强版：优先保留BENIGN）\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import dump\n",
    "import os\n",
    "\n",
    "# 加载标准字段\n",
    "with open(\"feature_columns.json\", \"r\") as f:\n",
    "    standard_columns = json.load(f)\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "os.makedirs(\"data/processed_npz\", exist_ok=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "        if \"Label\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        df[\"Label\"] = df[\"Label\"].astype(str)\n",
    "        benign_df = df[df[\"Label\"].str.upper() == \"BENIGN\"]\n",
    "        attack_df = df[df[\"Label\"].str.upper() != \"BENIGN\"]\n",
    "\n",
    "        # 采样策略：全保留 BENIGN + 随机攻击补足\n",
    "        max_rows = 100000\n",
    "        keep_benign = benign_df.copy()\n",
    "        need_attacks = max_rows - len(keep_benign)\n",
    "        sampled_attack = attack_df.sample(n=need_attacks, random_state=42) if need_attacks > 0 else attack_df.iloc[0:0]\n",
    "\n",
    "        df_sampled = pd.concat([keep_benign, sampled_attack]).sample(frac=1, random_state=42)\n",
    "\n",
    "        # 标签编码\n",
    "        y = df_sampled[\"Label\"].apply(lambda x: 0 if x.upper() == \"BENIGN\" else 1).values\n",
    "\n",
    "        # 特征处理\n",
    "        X = df_sampled.reindex(columns=standard_columns)\n",
    "        X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # 归一化（第一次拟合）\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # 保存\n",
    "        base = os.path.splitext(os.path.basename(path))[0]\n",
    "        np.savez_compressed(f\"data/processed_npz/{base}.npz\", X=X_scaled, y=y)\n",
    "        pd.DataFrame(X_scaled, columns=standard_columns).to_csv(f\"data/processed/{base}.csv\", index=False)\n",
    "        print(f\"✅ 清洗+采样: {base} | BENIGN: {len(keep_benign)}, ATTACK: {len(sampled_attack)}, 总: {len(y)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 错误处理: {path}, 原因: {e}\")\n",
    "\n",
    "# 保存归一化器\n",
    "dump(scaler, \"scaler.pkl\")\n",
    "print(\"✅ 已保存归一化器 scaler.pkl\")\n"
   ],
   "id": "b0f6c273da84448b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 清洗+采样: DrDoS_DNS | BENIGN: 3402, ATTACK: 96598, 总: 100000\n",
      "✅ 清洗+采样: DrDoS_LDAP | BENIGN: 1612, ATTACK: 98388, 总: 100000\n",
      "✅ 清洗+采样: DrDoS_MSSQL | BENIGN: 2006, ATTACK: 97994, 总: 100000\n",
      "✅ 清洗+采样: DrDoS_NetBIOS | BENIGN: 1707, ATTACK: 98293, 总: 100000\n",
      "✅ 清洗+采样: DrDoS_NTP | BENIGN: 14365, ATTACK: 85635, 总: 100000\n",
      "✅ 清洗+采样: DrDoS_SNMP | BENIGN: 1507, ATTACK: 98493, 总: 100000\n",
      "✅ 清洗+采样: DrDoS_SSDP | BENIGN: 763, ATTACK: 99237, 总: 100000\n",
      "✅ 清洗+采样: DrDoS_UDP | BENIGN: 2157, ATTACK: 97843, 总: 100000\n",
      "✅ 清洗+采样: Syn | BENIGN: 392, ATTACK: 99608, 总: 100000\n",
      "❌ 错误处理: data/raw/CICDDoS2019\\CSV-01-12\\01-12\\TFTP.csv, 原因: Error tokenizing data. C error: out of memory\n",
      "✅ 清洗+采样: UDPLag | BENIGN: 3705, ATTACK: 96295, 总: 100000\n",
      "✅ 清洗+采样: LDAP | BENIGN: 5124, ATTACK: 94876, 总: 100000\n",
      "✅ 清洗+采样: MSSQL | BENIGN: 2794, ATTACK: 97206, 总: 100000\n",
      "✅ 清洗+采样: NetBIOS | BENIGN: 1321, ATTACK: 98679, 总: 100000\n",
      "✅ 清洗+采样: Portmap | BENIGN: 4734, ATTACK: 95266, 总: 100000\n",
      "✅ 清洗+采样: Syn | BENIGN: 35790, ATTACK: 64210, 总: 100000\n",
      "✅ 清洗+采样: UDP | BENIGN: 3134, ATTACK: 96866, 总: 100000\n",
      "✅ 清洗+采样: UDPLag | BENIGN: 4068, ATTACK: 95932, 总: 100000\n",
      "✅ 已保存归一化器 scaler.pkl\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T06:36:51.605938Z",
     "start_time": "2025-05-09T06:36:28.020464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 1.4（修订版：基于 Timestamp 排序滑窗）\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "npz_dir = \"data/processed_npz\"\n",
    "csv_dir = \"data/processed\"\n",
    "out_dir = \"data/processed_npz_seq\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "window_size = 3\n",
    "step = 3\n",
    "\n",
    "npz_files = glob(os.path.join(npz_dir, \"*.npz\"))\n",
    "\n",
    "for npz_path in npz_files:\n",
    "    try:\n",
    "        base = os.path.splitext(os.path.basename(npz_path))[0]\n",
    "        csv_path = os.path.join(csv_dir, f\"{base}.csv\")\n",
    "\n",
    "        # 载入 CSV 保留时间顺序\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "        df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "        # 同时读取标签\n",
    "        data = np.load(npz_path)\n",
    "        y = data[\"y\"]\n",
    "\n",
    "        # 匹配 Timestamp 排序后的索引（可能丢弃无效时间）\n",
    "        valid_idx = df[\"Timestamp\"].notnull()\n",
    "        df = df[valid_idx]\n",
    "        y = y[valid_idx.values]\n",
    "\n",
    "        X = df.drop(columns=[\"Timestamp\"]).values  # 去掉时间戳列\n",
    "\n",
    "        # 滑窗切片\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(0, len(X) - window_size + 1, step):\n",
    "            window_y = y[i:i + window_size]\n",
    "            if np.all(window_y == window_y[0]):\n",
    "                X_seq.append(X[i:i + window_size])\n",
    "                y_seq.append(window_y[0])\n",
    "\n",
    "        np.savez_compressed(f\"{out_dir}/{base}_seq.npz\", X=np.array(X_seq), y=np.array(y_seq))\n",
    "        print(f\"✅ 时序切片: {base}, 有效序列: {len(y_seq)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 切片失败: {npz_path}, 原因: {e}\")\n"
   ],
   "id": "bc92106e96f8bea4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 时序切片: DrDoS_DNS, 有效序列: 30035\n",
      "✅ 时序切片: DrDoS_LDAP, 有效序列: 31752\n",
      "✅ 时序切片: DrDoS_MSSQL, 有效序列: 31371\n",
      "✅ 时序切片: DrDoS_NetBIOS, 有效序列: 31658\n",
      "✅ 时序切片: DrDoS_NTP, 有效序列: 21048\n",
      "✅ 时序切片: DrDoS_SNMP, 有效序列: 31855\n",
      "✅ 时序切片: DrDoS_SSDP, 有效序列: 32577\n",
      "✅ 时序切片: DrDoS_UDP, 有效序列: 31226\n",
      "✅ 时序切片: LDAP, 有效序列: 28468\n",
      "✅ 时序切片: MSSQL, 有效序列: 30612\n",
      "✅ 时序切片: NetBIOS, 有效序列: 32033\n",
      "✅ 时序切片: Portmap, 有效序列: 28820\n",
      "✅ 时序切片: Syn, 有效序列: 10371\n",
      "✅ 时序切片: UDP, 有效序列: 30292\n",
      "✅ 时序切片: UDPLag, 有效序列: 29425\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T06:37:16.494839Z",
     "start_time": "2025-05-09T06:37:04.998552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 1.5（修复：跳过类别不足文件）\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seq_dir = \"data/processed_npz_seq\"\n",
    "out_path = \"data/final_dataset\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_all, y_all = [], []\n",
    "\n",
    "for path in glob(os.path.join(seq_dir, \"*_seq.npz\")):\n",
    "    try:\n",
    "        data = np.load(path)\n",
    "        X, y = data[\"X\"], data[\"y\"]\n",
    "        if len(np.unique(y)) < 2:\n",
    "            print(f\"⚠️ 跳过: {os.path.basename(path)}，仅包含一个类别\")\n",
    "            continue\n",
    "        X_all.append(X)\n",
    "        y_all.append(y)\n",
    "        print(f\"✅ 载入: {os.path.basename(path)}, 序列数: {len(y)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 加载失败: {path}, 原因: {e}\")\n",
    "\n",
    "X_all = np.concatenate(X_all, axis=0)\n",
    "y_all = np.concatenate(y_all, axis=0)\n",
    "print(f\"📦 合并后样本数: {len(y_all)}, 输入形状: {X_all.shape}, 标签分布: {np.bincount(y_all)}\")\n",
    "\n",
    "# 划分数据集\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_all, y_all, test_size=0.15, random_state=42, stratify=y_all)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
    "\n",
    "# 保存\n",
    "np.savez_compressed(os.path.join(out_path, \"train.npz\"), X=X_train, y=y_train)\n",
    "np.savez_compressed(os.path.join(out_path, \"val.npz\"), X=X_val, y=y_val)\n",
    "np.savez_compressed(os.path.join(out_path, \"test.npz\"), X=X_test, y=y_test)\n",
    "print(\"✅ 划分并保存完毕：train/val/test.npz\")\n"
   ],
   "id": "c640515797c3e827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 载入: DrDoS_DNS_seq.npz, 序列数: 30035\n",
      "✅ 载入: DrDoS_LDAP_seq.npz, 序列数: 31752\n",
      "✅ 载入: DrDoS_MSSQL_seq.npz, 序列数: 31371\n",
      "✅ 载入: DrDoS_NetBIOS_seq.npz, 序列数: 31658\n",
      "✅ 载入: DrDoS_NTP_seq.npz, 序列数: 21048\n",
      "✅ 载入: DrDoS_SNMP_seq.npz, 序列数: 31855\n",
      "⚠️ 跳过: DrDoS_SSDP_seq.npz，仅包含一个类别\n",
      "✅ 载入: DrDoS_UDP_seq.npz, 序列数: 31226\n",
      "✅ 载入: LDAP_seq.npz, 序列数: 28468\n",
      "✅ 载入: MSSQL_seq.npz, 序列数: 30612\n",
      "✅ 载入: NetBIOS_seq.npz, 序列数: 32033\n",
      "✅ 载入: Portmap_seq.npz, 序列数: 28820\n",
      "✅ 载入: Syn_seq.npz, 序列数: 10371\n",
      "✅ 载入: UDPLag_seq.npz, 序列数: 29425\n",
      "✅ 载入: UDP_seq.npz, 序列数: 30292\n",
      "📦 合并后样本数: 398966, 输入形状: (398966, 3, 86), 标签分布: [  1693 397273]\n",
      "✅ 划分并保存完毕：train/val/test.npz\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:20:41.434527Z",
     "start_time": "2025-05-09T07:20:39.990921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 2.1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=86, hidden_dim=64, lstm_layers=1, dropout=0.3):\n",
    "        super(CNNLSTMClassifier, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T, F) → transpose for CNN\n",
    "        x = x.permute(0, 2, 1)  # (B, F, T)\n",
    "        x = self.cnn(x)         # (B, 128, T)\n",
    "        x = x.permute(0, 2, 1)  # (B, T, 128)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)     # (B, T, 2*hidden)\n",
    "        out = lstm_out[:, -1, :]       # 取最后一个时间步\n",
    "        out = self.classifier(out)     # (B, 1)\n",
    "        return out.squeeze(1)          # (B,)\n",
    "\n",
    "# 测试模型输出\n",
    "model = CNNLSTMClassifier()\n",
    "x_dummy = torch.randn(8, 3, 86)\n",
    "y_pred = model(x_dummy)\n",
    "print(\"✅ 模型输出形状:\", y_pred.shape)  # 应该是 [8]\n"
   ],
   "id": "482c3c9a3e014d9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型输出形状: torch.Size([8])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:20:50.394914Z",
     "start_time": "2025-05-09T07:20:43.719947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 2.2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# 配置\n",
    "BATCH_SIZE = 128\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✅ 当前设备:\", DEVICE)\n",
    "\n",
    "# 加载数据\n",
    "def load_dataset(path):\n",
    "    data = np.load(path)\n",
    "    X = torch.tensor(data[\"X\"], dtype=torch.float32)\n",
    "    y = torch.tensor(data[\"y\"], dtype=torch.float32)\n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "train_set = load_dataset(\"data/final_dataset/train.npz\")\n",
    "val_set = load_dataset(\"data/final_dataset/val.npz\")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 初始化模型、优化器、损失函数\n",
    "model = CNNLSTMClassifier().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCELoss()  # 输出为 Sigmoid 后概率\n",
    "\n",
    "print(f\"✅ 数据加载完成：训练样本 {len(train_set)}，验证样本 {len(val_set)}\")\n"
   ],
   "id": "6daf886c8b8a1697",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 当前设备: cuda\n",
      "✅ 数据加载完成：训练样本 279266，验证样本 59855\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:22:53.162156Z",
     "start_time": "2025-05-09T07:20:54.185172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 2.3\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "EPOCHS = 10\n",
    "BEST_F1 = 0.0\n",
    "save_path = \"model.pt\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    y_true_train, y_pred_train = [], []\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        y_true_train.extend(y_batch.cpu().numpy())\n",
    "        y_pred_train.extend(preds.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_true_train, y_pred_train)\n",
    "    print(f\"📘 Epoch {epoch} | Train Loss: {np.mean(train_losses):.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "\n",
    "    # 评估验证集\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    y_true_val, y_pred_val = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            y_true_val.extend(y_batch.cpu().numpy())\n",
    "            y_pred_val.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(y_true_val, y_pred_val)\n",
    "    val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "    print(f\"🧪 Validation | Loss: {np.mean(val_losses):.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "    # 保存最好模型\n",
    "    if val_f1 > BEST_F1:\n",
    "        BEST_F1 = val_f1\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"✅ Saved best model to {save_path}\")\n",
    "\n"
   ],
   "id": "943e6984ff6f4f96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 1 | Train Loss: 0.0177 | Acc: 0.9948 | F1: 0.9974\n",
      "🧪 Validation | Loss: 0.0136 | Acc: 0.9958 | F1: 0.9979\n",
      "✅ Saved best model to model.pt\n",
      "📘 Epoch 2 | Train Loss: 0.0139 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0135 | Acc: 0.9958 | F1: 0.9979\n",
      "📘 Epoch 3 | Train Loss: 0.0138 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0135 | Acc: 0.9958 | F1: 0.9979\n",
      "📘 Epoch 4 | Train Loss: 0.0138 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0135 | Acc: 0.9958 | F1: 0.9979\n",
      "📘 Epoch 5 | Train Loss: 0.0137 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0135 | Acc: 0.9958 | F1: 0.9979\n",
      "📘 Epoch 6 | Train Loss: 0.0136 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0136 | Acc: 0.9958 | F1: 0.9979\n",
      "📘 Epoch 7 | Train Loss: 0.0137 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0136 | Acc: 0.9958 | F1: 0.9979\n",
      "📘 Epoch 8 | Train Loss: 0.0137 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0136 | Acc: 0.9958 | F1: 0.9979\n",
      "📘 Epoch 9 | Train Loss: 0.0136 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0136 | Acc: 0.9958 | F1: 0.9979\n",
      "📘 Epoch 10 | Train Loss: 0.0137 | Acc: 0.9958 | F1: 0.9979\n",
      "🧪 Validation | Loss: 0.0136 | Acc: 0.9958 | F1: 0.9979\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T08:05:21.350312Z",
     "start_time": "2025-05-09T08:05:18.814418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 2.4\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 加载模型\n",
    "model = CNNLSTMClassifier().to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# 加载测试集\n",
    "test_data = np.load(\"data/final_dataset/test.npz\")\n",
    "X_test = torch.tensor(test_data[\"X\"], dtype=torch.float32).to(DEVICE)\n",
    "y_test = torch.tensor(test_data[\"y\"], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# 预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    preds = (outputs >= 0.5).float()\n",
    "\n",
    "# 计算准确率与 F1 值\n",
    "y_true = y_test.cpu().numpy()\n",
    "y_pred = preds.cpu().numpy()\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "test_f1 = f1_score(y_true, y_pred)\n",
    "test_precision = precision_score(y_true, y_pred)\n",
    "test_recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"📊 Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"📊 Test F1: {test_f1:.4f}\")\n",
    "print(f\"📊 Test Precision: {test_precision:.4f}\")\n",
    "print(f\"📊 Test Recall: {test_recall:.4f}\")\n"
   ],
   "id": "708ef08d19ad74be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Test Accuracy: 0.9958\n",
      "📊 Test F1: 0.9979\n",
      "📊 Test Precision: 0.9958\n",
      "📊 Test Recall: 1.0000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 代码块 2.5\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "# 随机选择一个 npz 文件\n",
    "npz_files = [f for f in os.listdir(\"data/final_dataset/\") if f.endswith(\".npz\")]\n",
    "random_file = random.choice(npz_files)\n",
    "\n",
    "# 加载选定的 npz 文件\n",
    "print(f\"🔄 正在加载文件: {random_file}\")\n",
    "data = np.load(os.path.join(\"data/final_dataset\", random_file))\n",
    "\n",
    "X_test = torch.tensor(data[\"X\"], dtype=torch.float32).to(DEVICE)\n",
    "y_test = torch.tensor(data[\"y\"], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# 预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    preds = (outputs >= 0.5).float()\n",
    "\n",
    "# 计算准确率与 F1 值\n",
    "y_true = y_test.cpu().numpy()\n",
    "y_pred = preds.cpu().numpy()\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "test_f1 = f1_score(y_true, y_pred)\n",
    "test_precision = precision_score(y_true, y_pred)\n",
    "test_recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"📊 Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"📊 Test F1: {test_f1:.4f}\")\n",
    "print(f\"📊 Test Precision: {test_precision:.4f}\")\n",
    "print(f\"📊 Test Recall: {test_recall:.4f}\")\n"
   ],
   "id": "ed629a580f0e0e4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T08:11:44.322992Z",
     "start_time": "2025-05-09T08:11:43.936749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码块 2.6（修正：从 data/processed_npz_seq 加载）\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 从合并前的数据集中随机选择一个 npz 文件\n",
    "npz_files = [f for f in os.listdir(\"data/processed_npz_seq\") if f.endswith(\".npz\")]\n",
    "random_file = random.choice(npz_files)\n",
    "\n",
    "# 加载选定的 npz 文件\n",
    "print(f\"🔄 正在加载文件: {random_file}\")\n",
    "data = np.load(os.path.join(\"data/processed_npz_seq\", random_file))\n",
    "\n",
    "X_test = torch.tensor(data[\"X\"], dtype=torch.float32).to(DEVICE)\n",
    "y_test = torch.tensor(data[\"y\"], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# 预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    preds = (outputs >= 0.5).float()\n",
    "\n",
    "# 计算准确率与 F1 值\n",
    "y_true = y_test.cpu().numpy()\n",
    "y_pred = preds.cpu().numpy()\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "test_f1 = f1_score(y_true, y_pred)\n",
    "test_precision = precision_score(y_true, y_pred)\n",
    "test_recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"📊 Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"📊 Test F1: {test_f1:.4f}\")\n",
    "print(f\"📊 Test Precision: {test_precision:.4f}\")\n",
    "print(f\"📊 Test Recall: {test_recall:.4f}\")\n"
   ],
   "id": "754e49f4acef63b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 正在加载文件: UDP_seq.npz\n",
      "📊 Test Accuracy: 1.0000\n",
      "📊 Test F1: 1.0000\n",
      "📊 Test Precision: 1.0000\n",
      "📊 Test Recall: 1.0000\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ddos",
   "language": "python",
   "display_name": "Python (DDOS)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
