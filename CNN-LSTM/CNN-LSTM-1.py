#%%
# ä»£ç å— 1.1

import os
import pandas as pd
from glob import glob
from collections import Counter

# è®¾ç½®åŸå§‹æ•°æ®è·¯å¾„
data_dir = "data/raw/CICDDoS2019"
csv_files = glob(os.path.join(data_dir, "**/*.csv"), recursive=True)

column_counter = Counter()
label_summary = {}

for path in csv_files:
    try:
        df = pd.read_csv(path, nrows=5000)  # è¯»å–å‰5kè¡Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
        filename = os.path.basename(path)
        df.columns = [col.strip() for col in df.columns]  # å»é™¤åˆ—åç©ºæ ¼
        print(f"âœ… æ–‡ä»¶: {filename}")
        print(f"   â–¶ï¸ è¡Œæ•°: {df.shape[0]}, åˆ—æ•°: {df.shape[1]}")
        print(f"   ğŸ§± ç¼ºå¤±å€¼æ€»æ•°: {df.isnull().sum().sum()}")

        # ç´¯åŠ å®Œæ•´åˆ—åé¢‘æ¬¡
        column_counter.update(df.columns)

        # æ ‡ç­¾åˆ—ç»Ÿè®¡
        label_col = [col for col in df.columns if 'label' in col.lower() or 'attack' in col.lower()]
        if label_col:
            label_counts = df[label_col[0]].value_counts()
            label_summary[filename] = label_counts.to_dict()
            print(f"   ğŸ·ï¸ æ ‡ç­¾åˆ—: {label_col[0]}, åˆ†å¸ƒ: {label_counts.to_dict()}")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°æ ‡ç­¾åˆ—")
        print("-" * 60)
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {path}, åŸå› : {e}")

# è¾“å‡ºæ‰€æœ‰åˆ—ååŠé¢‘æ¬¡
print("\nğŸ“Š æ‰€æœ‰æ ‡å‡†åŒ–åˆ—ååŠå‡ºç°é¢‘æ¬¡ï¼ˆå…± {} ä¸ªï¼‰ï¼š".format(len(column_counter)))
for col, count in column_counter.most_common():
    print(f"{col}: {count}")

#%%
# ä»£ç å— 1.2

import json

# æ’é™¤æ‰æ ‡ç­¾åˆ—
standard_columns = [col for col in column_counter if col != "Label"]

# ä¿å­˜ä¸º JSON æ–‡ä»¶
with open("feature_columns.json", "w", encoding="utf-8") as f:
    json.dump(standard_columns, f, indent=2)

print(f"âœ… å·²ä¿å­˜æ ‡å‡†å­—æ®µåˆ— {len(standard_columns)} é¡¹ åˆ° feature_columns.json")

#%%
# ä»£ç å— 1.3ï¼ˆæœ€ç»ˆå¢å¼ºç‰ˆï¼šä¼˜å…ˆä¿ç•™BENIGNï¼‰

import numpy as np
from sklearn.preprocessing import MinMaxScaler
from joblib import dump
import os

# åŠ è½½æ ‡å‡†å­—æ®µ
with open("feature_columns.json", "r") as f:
    standard_columns = json.load(f)

os.makedirs("data/processed", exist_ok=True)
os.makedirs("data/processed_npz", exist_ok=True)

scaler = MinMaxScaler()

for path in csv_files:
    try:
        df = pd.read_csv(path, low_memory=False)
        df.columns = [col.strip() for col in df.columns]

        if "Label" not in df.columns:
            continue

        df["Label"] = df["Label"].astype(str)
        benign_df = df[df["Label"].str.upper() == "BENIGN"]
        attack_df = df[df["Label"].str.upper() != "BENIGN"]

        # é‡‡æ ·ç­–ç•¥ï¼šå…¨ä¿ç•™ BENIGN + éšæœºæ”»å‡»è¡¥è¶³
        max_rows = 100000
        keep_benign = benign_df.copy()
        need_attacks = max_rows - len(keep_benign)
        sampled_attack = attack_df.sample(n=need_attacks, random_state=42) if need_attacks > 0 else attack_df.iloc[0:0]

        df_sampled = pd.concat([keep_benign, sampled_attack]).sample(frac=1, random_state=42)

        # æ ‡ç­¾ç¼–ç 
        y = df_sampled["Label"].apply(lambda x: 0 if x.upper() == "BENIGN" else 1).values

        # ç‰¹å¾å¤„ç†
        X = df_sampled.reindex(columns=standard_columns)
        X = X.apply(pd.to_numeric, errors="coerce")
        X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

        # å½’ä¸€åŒ–ï¼ˆç¬¬ä¸€æ¬¡æ‹Ÿåˆï¼‰
        X_scaled = scaler.fit_transform(X)

        # ä¿å­˜
        base = os.path.splitext(os.path.basename(path))[0]
        np.savez_compressed(f"data/processed_npz/{base}.npz", X=X_scaled, y=y)
        pd.DataFrame(X_scaled, columns=standard_columns).to_csv(f"data/processed/{base}.csv", index=False)
        print(f"âœ… æ¸…æ´—+é‡‡æ ·: {base} | BENIGN: {len(keep_benign)}, ATTACK: {len(sampled_attack)}, æ€»: {len(y)}")

    except Exception as e:
        print(f"âŒ é”™è¯¯å¤„ç†: {path}, åŸå› : {e}")

# ä¿å­˜å½’ä¸€åŒ–å™¨
dump(scaler, "scaler.pkl")
print("âœ… å·²ä¿å­˜å½’ä¸€åŒ–å™¨ scaler.pkl")

#%%
# ä»£ç å— 1.4ï¼ˆä¿®è®¢ç‰ˆï¼šåŸºäº Timestamp æ’åºæ»‘çª—ï¼‰

import numpy as np
import os
import pandas as pd
from glob import glob

npz_dir = "data/processed_npz"
csv_dir = "data/processed"
out_dir = "data/processed_npz_seq"
os.makedirs(out_dir, exist_ok=True)

window_size = 3
step = 3

npz_files = glob(os.path.join(npz_dir, "*.npz"))

for npz_path in npz_files:
    try:
        base = os.path.splitext(os.path.basename(npz_path))[0]
        csv_path = os.path.join(csv_dir, f"{base}.csv")

        # è½½å…¥ CSV ä¿ç•™æ—¶é—´é¡ºåº
        df = pd.read_csv(csv_path)
        df["Timestamp"] = pd.to_datetime(df["Timestamp"], errors="coerce")
        df = df.sort_values("Timestamp").reset_index(drop=True)

        # åŒæ—¶è¯»å–æ ‡ç­¾
        data = np.load(npz_path)
        y = data["y"]

        # åŒ¹é… Timestamp æ’åºåçš„ç´¢å¼•ï¼ˆå¯èƒ½ä¸¢å¼ƒæ— æ•ˆæ—¶é—´ï¼‰
        valid_idx = df["Timestamp"].notnull()
        df = df[valid_idx]
        y = y[valid_idx.values]

        X = df.drop(columns=["Timestamp"]).values  # å»æ‰æ—¶é—´æˆ³åˆ—

        # æ»‘çª—åˆ‡ç‰‡
        X_seq, y_seq = [], []
        for i in range(0, len(X) - window_size + 1, step):
            window_y = y[i:i + window_size]
            if np.all(window_y == window_y[0]):
                X_seq.append(X[i:i + window_size])
                y_seq.append(window_y[0])

        np.savez_compressed(f"{out_dir}/{base}_seq.npz", X=np.array(X_seq), y=np.array(y_seq))
        print(f"âœ… æ—¶åºåˆ‡ç‰‡: {base}, æœ‰æ•ˆåºåˆ—: {len(y_seq)}")

    except Exception as e:
        print(f"âŒ åˆ‡ç‰‡å¤±è´¥: {npz_path}, åŸå› : {e}")

#%%
# ä»£ç å— 1.5ï¼ˆä¿®å¤ï¼šè·³è¿‡ç±»åˆ«ä¸è¶³æ–‡ä»¶ï¼‰

import numpy as np
from glob import glob
import os
from sklearn.model_selection import train_test_split

seq_dir = "data/processed_npz_seq"
out_path = "data/final_dataset"
os.makedirs(out_path, exist_ok=True)

X_all, y_all = [], []

for path in glob(os.path.join(seq_dir, "*_seq.npz")):
    try:
        data = np.load(path)
        X, y = data["X"], data["y"]
        if len(np.unique(y)) < 2:
            print(f"âš ï¸ è·³è¿‡: {os.path.basename(path)}ï¼Œä»…åŒ…å«ä¸€ä¸ªç±»åˆ«")
            continue
        X_all.append(X)
        y_all.append(y)
        print(f"âœ… è½½å…¥: {os.path.basename(path)}, åºåˆ—æ•°: {len(y)}")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {path}, åŸå› : {e}")

X_all = np.concatenate(X_all, axis=0)
y_all = np.concatenate(y_all, axis=0)
print(f"ğŸ“¦ åˆå¹¶åæ ·æœ¬æ•°: {len(y_all)}, è¾“å…¥å½¢çŠ¶: {X_all.shape}, æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_all)}")

# åˆ’åˆ†æ•°æ®é›†
X_temp, X_test, y_temp, y_test = train_test_split(X_all, y_all, test_size=0.15, random_state=42, stratify=y_all)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)

# ä¿å­˜
np.savez_compressed(os.path.join(out_path, "train.npz"), X=X_train, y=y_train)
np.savez_compressed(os.path.join(out_path, "val.npz"), X=X_val, y=y_val)
np.savez_compressed(os.path.join(out_path, "test.npz"), X=X_test, y=y_test)
print("âœ… åˆ’åˆ†å¹¶ä¿å­˜å®Œæ¯•ï¼štrain/val/test.npz")

#%%
# ä»£ç å— 2.1

import torch
import torch.nn as nn

class CNNLSTMClassifier(nn.Module):
    def __init__(self, input_dim=86, hidden_dim=64, lstm_layers=1, dropout=0.3):
        super(CNNLSTMClassifier, self).__init__()

        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(dropout)
        )

        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_dim,
                            num_layers=lstm_layers, batch_first=True, bidirectional=True)

        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x shape: (B, T, F) â†’ transpose for CNN
        x = x.permute(0, 2, 1)  # (B, F, T)
        x = self.cnn(x)         # (B, 128, T)
        x = x.permute(0, 2, 1)  # (B, T, 128)

        lstm_out, _ = self.lstm(x)     # (B, T, 2*hidden)
        out = lstm_out[:, -1, :]       # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        out = self.classifier(out)     # (B, 1)
        return out.squeeze(1)          # (B,)

# æµ‹è¯•æ¨¡å‹è¾“å‡º
model = CNNLSTMClassifier()
x_dummy = torch.randn(8, 3, 86)
y_pred = model(x_dummy)
print("âœ… æ¨¡å‹è¾“å‡ºå½¢çŠ¶:", y_pred.shape)  # åº”è¯¥æ˜¯ [8]

#%%
# ä»£ç å— 2.2

import torch
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# é…ç½®
BATCH_SIZE = 128
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… å½“å‰è®¾å¤‡:", DEVICE)

# åŠ è½½æ•°æ®
def load_dataset(path):
    data = np.load(path)
    X = torch.tensor(data["X"], dtype=torch.float32)
    y = torch.tensor(data["y"], dtype=torch.float32)
    return TensorDataset(X, y)

train_set = load_dataset("data/final_dataset/train.npz")
val_set = load_dataset("data/final_dataset/val.npz")

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)

# åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°
model = CNNLSTMClassifier().to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = torch.nn.BCELoss()  # è¾“å‡ºä¸º Sigmoid åæ¦‚ç‡

print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼šè®­ç»ƒæ ·æœ¬ {len(train_set)}ï¼ŒéªŒè¯æ ·æœ¬ {len(val_set)}")

#%%
# ä»£ç å— 2.3

import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import os

EPOCHS = 10
BEST_F1 = 0.0
save_path = "model.pt"

for epoch in range(1, EPOCHS + 1):
    model.train()
    train_losses = []
    y_true_train, y_pred_train = [], []

    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        train_losses.append(loss.item())
        preds = (outputs >= 0.5).float()
        y_true_train.extend(y_batch.cpu().numpy())
        y_pred_train.extend(preds.cpu().numpy())

    train_acc = accuracy_score(y_true_train, y_pred_train)
    train_f1 = f1_score(y_true_train, y_pred_train)
    print(f"ğŸ“˜ Epoch {epoch} | Train Loss: {np.mean(train_losses):.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}")

    # è¯„ä¼°éªŒè¯é›†
    model.eval()
    val_losses = []
    y_true_val, y_pred_val = [], []

    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            val_losses.append(loss.item())
            preds = (outputs >= 0.5).float()
            y_true_val.extend(y_batch.cpu().numpy())
            y_pred_val.extend(preds.cpu().numpy())

    val_f1 = f1_score(y_true_val, y_pred_val)
    val_acc = accuracy_score(y_true_val, y_pred_val)
    print(f"ğŸ§ª Validation | Loss: {np.mean(val_losses):.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}")

    # ä¿å­˜æœ€å¥½æ¨¡å‹
    if val_f1 > BEST_F1:
        BEST_F1 = val_f1
        torch.save(model.state_dict(), save_path)
        print(f"âœ… Saved best model to {save_path}")


#%%
# ä»£ç å— 2.4

import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# åŠ è½½æ¨¡å‹
model = CNNLSTMClassifier().to(DEVICE)
model.load_state_dict(torch.load("model.pt"))
model.eval()

# åŠ è½½æµ‹è¯•é›†
test_data = np.load("data/final_dataset/test.npz")
X_test = torch.tensor(test_data["X"], dtype=torch.float32).to(DEVICE)
y_test = torch.tensor(test_data["y"], dtype=torch.float32).to(DEVICE)

# é¢„æµ‹
with torch.no_grad():
    outputs = model(X_test)
    preds = (outputs >= 0.5).float()

# è®¡ç®—å‡†ç¡®ç‡ä¸ F1 å€¼
y_true = y_test.cpu().numpy()
y_pred = preds.cpu().numpy()

test_acc = accuracy_score(y_true, y_pred)
test_f1 = f1_score(y_true, y_pred)
test_precision = precision_score(y_true, y_pred)
test_recall = recall_score(y_true, y_pred)

print(f"ğŸ“Š Test Accuracy: {test_acc:.4f}")
print(f"ğŸ“Š Test F1: {test_f1:.4f}")
print(f"ğŸ“Š Test Precision: {test_precision:.4f}")
print(f"ğŸ“Š Test Recall: {test_recall:.4f}")

#%%
# ä»£ç å— 2.5

import os
import random

# éšæœºé€‰æ‹©ä¸€ä¸ª npz æ–‡ä»¶
npz_files = [f for f in os.listdir("data/final_dataset/") if f.endswith(".npz")]
random_file = random.choice(npz_files)

# åŠ è½½é€‰å®šçš„ npz æ–‡ä»¶
print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ–‡ä»¶: {random_file}")
data = np.load(os.path.join("data/final_dataset", random_file))

X_test = torch.tensor(data["X"], dtype=torch.float32).to(DEVICE)
y_test = torch.tensor(data["y"], dtype=torch.float32).to(DEVICE)

# é¢„æµ‹
with torch.no_grad():
    outputs = model(X_test)
    preds = (outputs >= 0.5).float()

# è®¡ç®—å‡†ç¡®ç‡ä¸ F1 å€¼
y_true = y_test.cpu().numpy()
y_pred = preds.cpu().numpy()

test_acc = accuracy_score(y_true, y_pred)
test_f1 = f1_score(y_true, y_pred)
test_precision = precision_score(y_true, y_pred)
test_recall = recall_score(y_true, y_pred)

print(f"ğŸ“Š Test Accuracy: {test_acc:.4f}")
print(f"ğŸ“Š Test F1: {test_f1:.4f}")
print(f"ğŸ“Š Test Precision: {test_precision:.4f}")
print(f"ğŸ“Š Test Recall: {test_recall:.4f}")

#%%
# ä»£ç å— 2.6ï¼ˆä¿®æ­£ï¼šä» data/processed_npz_seq åŠ è½½ï¼‰

import random
import numpy as np
import torch
import os

# ä»åˆå¹¶å‰çš„æ•°æ®é›†ä¸­éšæœºé€‰æ‹©ä¸€ä¸ª npz æ–‡ä»¶
npz_files = [f for f in os.listdir("data/processed_npz_seq") if f.endswith(".npz")]
random_file = random.choice(npz_files)

# åŠ è½½é€‰å®šçš„ npz æ–‡ä»¶
print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ–‡ä»¶: {random_file}")
data = np.load(os.path.join("data/processed_npz_seq", random_file))

X_test = torch.tensor(data["X"], dtype=torch.float32).to(DEVICE)
y_test = torch.tensor(data["y"], dtype=torch.float32).to(DEVICE)

# é¢„æµ‹
with torch.no_grad():
    outputs = model(X_test)
    preds = (outputs >= 0.5).float()

# è®¡ç®—å‡†ç¡®ç‡ä¸ F1 å€¼
y_true = y_test.cpu().numpy()
y_pred = preds.cpu().numpy()

test_acc = accuracy_score(y_true, y_pred)
test_f1 = f1_score(y_true, y_pred)
test_precision = precision_score(y_true, y_pred)
test_recall = recall_score(y_true, y_pred)

print(f"ğŸ“Š Test Accuracy: {test_acc:.4f}")
print(f"ğŸ“Š Test F1: {test_f1:.4f}")
print(f"ğŸ“Š Test Precision: {test_precision:.4f}")
print(f"ğŸ“Š Test Recall: {test_recall:.4f}")
